---
tags:
  - #MISC
  - #NOT
cssclasses:
  - center-titles
  - embed-auto
---

# Making Neural Programming Architectures Generalize Via Recursion

## 1. Identificaci贸n del Problema

> [!info] Problema Central
> Las arquitecturas neuronales existentes para aprender programas a partir de datos **no generalizan bien** m谩s all谩 de un cierto nivel de complejidad de entrada, y es dif铆cil razonar sobre su comportamiento en situaciones no vistas.

| Problema                                   | Causa Principal                                                       |
| ------------------------------------------ | --------------------------------------------------------------------- |
| Generalizaci贸n pobre en entradas complejas | Dependencias espurias en estados ocultos de alta dimensionalidad      |
| Falta de interpretabilidad                 | Comportamiento no razonable m谩s all谩 de los ejemplos de entrenamiento |
| Sin garant铆as comprobables                 | Arquitecturas no dise帽adas para permitir verificaci贸n formal          |

> [!example] Ejemplo en Tareas
> Modelos entrenados en sumas de 3 d铆gitos fallan al generalizar a sumas de 5000 d铆gitos, a pesar de usar *curriculum learning*.

---

## 2. Revisi贸n de Informaci贸n Relacionada

> [!info] Modelos Previos
> Se han propuesto m煤ltiples arquitecturas para aprender programas, pero ninguna incorpora recursi贸n de manera expl铆cita.

| Modelo                              | A帽o  | Caracter铆stica Principal               |
| ----------------------------------- | ---- | -------------------------------------- |
| Neural Turing Machine               | 2014 | Memoria externa diferenciable          |
| Neural GPU                          | 2015 | Aprendizaje de algoritmos con GPU      |
| Neural Programmer-Interpreter (NPI) | 2016 | Trazas de ejecuci贸n sint茅ticas         |
| Pointer Networks                    | 2015 | Mecanismos de atenci贸n para secuencias |
| Hierarchical Attentive Memory       | 2016 | Memoria jer谩rquica y atenci贸n          |

> [!warning] Limitaciones Comunes
> Ninguno de estos modelos provee **garant铆as comprobables** de generalizaci贸n perfecta, y todos muestran degradaci贸n en entradas m谩s complejas.

---

## 3. Formulaci贸n de Hip贸tesis

> [!success] Hip贸tesis Principal
> La **incorporaci贸n expl铆cita de recursi贸n** en arquitecturas neuronales permite:
> - Mejor generalizaci贸n
> - Reducci贸n de la complejidad del problema
> - **Garant铆as comprobables** de generalizaci贸n perfecta

> [!info] Fundamentos de la Recursi贸n
> - **Base cases**: Escenarios de terminaci贸n sin recursi贸n
> - **Reduction rules**: Reglas que reducen problemas hacia los casos base
> - La recursi贸n permite dividir el problema en instancias m谩s peque帽as y manejables

---

## 4. Identificaci贸n de Datos Necesarios

> [!info] Requisitos de Datos
> Para entrenar y verificar programas neuronales recursivos se necesitan:

| Tipo de Dato                      | Descripci贸n                                                             |
| --------------------------------- | ----------------------------------------------------------------------- |
| Trazas de ejecuci贸n sint茅ticas    | Secuencias de entrada-salida que capturen comportamiento recursivo      |
| Conjuntos de verificaci贸n         | Entradas que cubran todos los casos base y reglas de reducci贸n          |
| Problemas de complejidad variable | Desde ejemplos simples hasta casos complejos no vistos en entrenamiento |

> [!example] Tareas Espec铆ficas
> - Suma de n煤meros de m煤ltiples d铆gitos
> - Algoritmos de ordenamiento (Bubble Sort, Quicksort)
> - Ordenamiento topol贸gico en grafos (DAGs)

---

## 5. Recolecci贸n de Datos

> [!success] Estrategia de Recolecci贸n
> Se generaron trazas de entrenamiento sint茅ticas para cada tarea:

| Tarea            | Cantidad de Trazas | Complejidad M谩xima en Entrenamiento |
| ---------------- | ------------------ | ----------------------------------- |
| Suma             | 200                | 3 d铆gitos                           |
| Bubble Sort      | 100                | Arrays de longitud 2                |
| Topological Sort | 6                  | Grafos de 5-7 nodos                 |
| Quicksort        | 4                  | Arrays de longitud 5                |

> [!warning] Nota Importante
> Los mismos problemas se usaron para generar trazas tanto en versiones recursivas como no recursivas, permitiendo comparaci贸n justa.

---

## 6. An谩lisis de Datos

> [!success] Resultados de Generalizaci贸n
> Los modelos recursivos mostraron **100% de precisi贸n** en todas las tareas, incluso en entradas mucho m谩s complejas que las de entrenamiento.

**Tabla 1: Bubble Sort - Precisi贸n por Longitud del Array**

| Longitud | No Recursivo | Parcial Recursivo | Total Recursivo |
| -------- | ------------ | ----------------- | --------------- |
| 2        | 100%         | 100%              | 100%            |
| 3        | 6.7%         | 23%               | 100%            |
| 8        | 0%           | 0%                | 100%            |
| 90       | 0%           | 0%                | 100%            |

**Tabla 2: Quicksort - Precisi贸n por Longitud del Array**

| Longitud | No Recursivo | Recursivo |
| -------- | ------------ | --------- |
| 11       | 73.3%        | 100%      |
| 20       | 30%          | 100%      |
| 70       | 0%           | 100%      |

> [!info] Interpretaci贸n
> Los modelos no recursivos aprendieron dependencias espurias relacionadas con la longitud de entrada, mientras que los recursivos capturaron la **sem谩ntica verdadera del programa**.

---

## 7. Resumen de Resultados

> [!success] Contribuciones Principales
> - **Primera implementaci贸n** de programas neuronales recursivos
> - **Generalizaci贸n perfecta** demostrada emp铆ricamente
> - **Verificaci贸n formal** mediante conjuntos de verificaci贸n finitos
> - **Reducci贸n dr谩stica** en datos de entrenamiento necesarios

> [!example] Caso Destacado
> En topological sort, el modelo recursivo entrenado con **solo 1 traza** generaliz贸 perfectamente a grafos de 70 nodos, mientras que el no recursivo fall贸 completamente.

---

## 8. Conclusiones e Implicaciones

> [!success] Conclusi贸n Principal
> La **recursi贸n es esencial** para que las arquitecturas neuronales aprendan programas que generalicen robustamente y permitan **garant铆as comprobables** de comportamiento correcto.

> [!info] Implicaciones Pr谩cticas
> - Posibilidad de usar programas neuronales en aplicaciones cr铆ticas
> - Reducci贸n significativa en la cantidad de datos de entrenamiento necesarios
> - Mayor interpretabilidad y capacidad de razonamiento sobre el comportamiento

> [!tip] Trabajo Futuro
> - Reducir supervisi贸n (trazas parciales o no recursivas)
> - Explorar m谩s tareas con estructura recursiva
> - Desarrollar arquitecturas integradas directamente con recursi贸n

> [!quote] Cita Clave
> *"Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system's behavior."*